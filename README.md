# ðŸ¤– AI Document Processing Agent

> **A production-grade, asynchronous document processing pipeline built with FastAPI, LangGraph, and CrewAI.**

This system ingests unstructured documents (Invoices, Contracts, Emails), intelligently classifies them using LLM agents, extracts structured JSON data, and handles edge cases with smart routing logic.

---

## ðŸš€ Features

* **âš¡ Asynchronous Architecture**: Powered by **Celery** & **Redis** to decouple API ingestion from heavy AI processing, ensuring high concurrency and non-blocking responses.
* **ðŸ§  Intelligent "Skeptical Auditor"**: Uses **Chain-of-Thought (CoT)** prompting to distinguish between actual business documents and informal emails *discussing* them.
* **ðŸ”€ Smart Routing (LangGraph)**:
    * **High Confidence (â‰¥70%)**: Automatically routed to specialized extraction agents.
    * **Low Confidence (<70%)**: Flagged and routed to a "Manual Review" queue, optimizing accuracy and ensuring safety.
* **ðŸ“Š Structured Extraction**: Enforces strict JSON schemas for all outputs, converting messy text into reliable data.
* **ðŸ›¡ï¸ Robust Error Handling**: Gracefully handles empty files, vague content, and JSON parsing errors without crashing.

---

## ðŸ”„ Architecture & Workflow

1.  **Ingestion ðŸ“¥**: User uploads a file via **FastAPI** (Non-blocking). The task is immediately pushed to **Redis**.
2.  **Async Processing âš™ï¸**: A background **Celery Worker** picks up the task from the queue.
3.  **Smart Classification (The Brain) ðŸ§ **:
    * The **Classification Agent** analyzes the text using Llama 3.1.
    * *Logic Check:* It asks, "Is this the document itself, or just an email talking about it?"
    * Assigns a `Classification` and a `Confidence Score`.
4.  **Conditional Routing (LangGraph) ðŸ”€**:
    * âœ… **Score â‰¥ 70%**: Routes to the specific **Extractor Agent** (e.g., `Invoice Extractor`).
    * âš ï¸ **Score < 70%**: Routes to the **Manual Review** node.
5.  **Extraction ðŸ“**: Returns structured JSON (Pydantic-validated) containing fields like `Total`, `Vendor`, `Dates`, etc.

---

## ðŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
| :--- | :--- | :--- |
| **API Layer** | **FastAPI** | High-performance REST API for file uploads. |
| **Orchestration** | **LangGraph** | Stateful workflow management and conditional routing. |
| **Agents** | **CrewAI** | Agentic logic and LLM interaction (via Groq). |
| **Queue** | **Celery** | Distributed task queue for async background jobs. |
| **Broker** | **Redis** | Message broker and result backend. |

---

## ðŸ“‹ Prerequisites

Before running the system, ensure you have the following installed:

* **Python 3.9+**
* **Redis Server** (Running locally on default port `6379`)

---

## âš™ï¸ Installation

1.  **Clone the Repository**
    ```bash
    git clone [https://github.com/Devarsh009/ai-document-processing-agent.git](https://github.com/Devarsh009/ai-document-processing-agent.git)
    cd ai-document-processing-agent
    ```

2.  **Create Virtual Environment**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate

    # Mac/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install Dependencies**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure Environment**
    Create a `.env` file in the root directory:
    ```env
    GROQ_API_KEY=gsk_your_actual_api_key_here
    ```

---

## ðŸƒâ€â™‚ï¸ How to Run

To run the full pipeline, you need **three separate terminal windows**:

### 1ï¸âƒ£ Start Redis
Ensure your local Redis server is active.
```bash
redis-server
2ï¸âƒ£ Start Celery Worker
This background worker listens for and executes AI tasks.

Bash

# Windows (Use --pool=solo)
python -m celery -A app.workers.celery_app:celery_app worker --loglevel=info --pool=solo

# Mac/Linux
python -m celery -A app.workers.celery_app:celery_app worker --loglevel=info
3ï¸âƒ£ Start FastAPI Server
This launches the REST API at http://127.0.0.1:8000.

Bash

uvicorn app.main:app --reload
ðŸ§ª Testing the System
Option A: Automated Concurrent Test âš¡
Use the included script to simulate multiple users uploading different documents simultaneously.

Ensure sample files exist in the test_samples/ folder.

Run the script:

Bash

python test_script.py
Watch the Magic: Check your Celery Terminal to see agents processing files in parallel!

Option B: Manual API Testing ðŸ–ï¸
Open the interactive Swagger UI: http://127.0.0.1:8000/docs

Use the POST /process endpoint.

Upload a .txt file.

The API returns a Task ID. Check the Celery logs for the JSON result.

ðŸ“‚ Project Structure
Plaintext

AI_PROJECT/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ agents/           # ðŸ¤– CrewAI Agent definitions (Classifier, Extractor)
â”‚   â”œâ”€â”€ workers/          # âš™ï¸ Celery task configuration
â”‚   â”œâ”€â”€ workflows/        # ðŸ”€ LangGraph nodes & conditional logic
â”‚   â”œâ”€â”€ utils/            # ðŸ“ Logging & helpers
â”‚   â””â”€â”€ main.py           # ðŸŒ FastAPI entry point
â”œâ”€â”€ test_samples/         # ðŸ“„ Sample documents (Invoice, Contract, Email)
â”œâ”€â”€ uploads/              # ðŸ“‚ Temp storage for processing
â”œâ”€â”€ test_script.py        # ðŸ§ª Concurrent testing tool
â”œâ”€â”€ requirements.txt      # ðŸ“¦ Pinned dependencies
â””â”€â”€ README.md             # ðŸ“– Documentation
ðŸ›¡ï¸ Edge Case Handling
The system is designed to handle real-world messiness:

Vague/Conversational Files: Caught by the "Skeptical Auditor" classifier logic (low confidence) and routed to Manual Review.

Hallucinations: Strict prompt engineering enforces JSON-only responses.

JSON Errors: A dedicated cleaning utility strips Markdown formatting before parsing.

Author: Devarsh